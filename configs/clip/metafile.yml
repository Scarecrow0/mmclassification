Collection:
  - Name: CLIP
    Metadata:
      Architecture:
        - Attention Dropout
        - Convolution
        - Dense Connections
        - Dropout
        - GELU
        - Layer Normalization
        - Multi-Head Attention
        - Scaled Dot-Product Attention
        - Tanh Activation
    Paper:
      URL: https://arxiv.org/abs/2201.09792
      Title: Learning Transferable Visual Models From Natural Language Supervision
    README: configs/clip/README.md
Models:
  - Name: clip-vit-base-p16_laion2b_ft_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 49370078208.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 86.52
          Top 5 Accuracy: 97.97
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_384.laion2b_ft_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_laion2b_ft_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 4364335104.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 82.46
          Top 5 Accuracy: 96.12
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_224.laion2b_ft_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_openai_ft_in12k_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 49370078208.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 86.87
          Top 5 Accuracy: 98.05
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_384.openai_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_openai_ft_in12k_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 16855600128.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.99
          Top 5 Accuracy: 97.72
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_224.openai_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_laion2b_ft_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 16855600128.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.49
          Top 5 Accuracy: 97.59
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_224.laion2b_ft_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_laion2b_ft_in12k_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 49370078208.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 87.17
          Top 5 Accuracy: 98.02
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_openai_ft_in12k_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 12661054464.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.13
          Top 5 Accuracy: 97.42
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_384.openai_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_384.openai_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_laion2b_ft_in12k_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 16855600128.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 86.02
          Top 5 Accuracy: 97.76
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_openai_ft_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 4364335104.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 81.77
          Top 5 Accuracy: 95.89
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_224.openai_ft_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_224.openai_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_openai_ft_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 16855600128.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.3
          Top 5 Accuracy: 97.5
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_224.openai_ft_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p16_openai_ft_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 49370078208.0
      Parameters: 86568424
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 86.25
          Top 5 Accuracy: 97.9
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch16_clip_384.openai_ft_in1k.bin
    Config: configs/clip/vit-base-p16_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_laion2b_ft_in12k_in1k-ft_in1k-224px
    Metadata:
      FLOPs: 4364335104.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 83.06
          Top 5 Accuracy: 96.49
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-224.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_laion2b_ft_in12k_in1k-ft_in1k-448px
    Metadata:
      FLOPs: 17202416640.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.76
          Top 5 Accuracy: 97.63
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-448.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models

  - Name: clip-vit-base-p32_laion2b_ft_in12k_in1k-ft_in1k-384px
    Metadata:
      FLOPs: 12661054464.0
      Parameters: 88225000
    In Collection: CLIP
    Results:
      - Dataset: ImageNet-1k
        Metrics:
          Top 1 Accuracy: 85.39
          Top 5 Accuracy: 97.67
        Task: Image Classification
    Weights: /mnt/petrelfs/share_data/lirongjie/clip_backbone_ckpt/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k.bin
    Config: configs/clip/vit-base-p32_pt-64xb64_in1k-384.py
    Converted From:
      Weights: https://huggingface.co/timm/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k
      Code: https://github.com/rwightman/pytorch-image-models
